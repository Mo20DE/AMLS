{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92134649",
      "metadata": {
        "id": "92134649"
      },
      "source": [
        "# Task 1.1 - Dataset Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd1fbbb",
      "metadata": {
        "id": "6fd1fbbb"
      },
      "source": [
        "### Load training data in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e099385e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from utils import TimeSeriesDataset, collate_fn, load_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XT0TGlidjfTm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT0TGlidjfTm",
        "outputId": "15211af3-86f9-4913-fd31-b976c2b04c2e"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://drive.google.com/drive/folders/1MyX__3hRiPkWHGFKqZXpOyq32KQpPDTR?usp=sharing\"\n",
        "gdown.download_folder(data_url, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d127ecbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "ecg_data_path = 'data/X_train.bin'\n",
        "labels_path = 'data/y_train.csv'\n",
        "\n",
        "ecg_data = load_data(ecg_data_path, 'rb')\n",
        "labels = load_data(labels_path, 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43879e12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "43879e12",
        "outputId": "76fd0b84-9efe-4f4c-c11f-6027ecdb4389"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Data': ecg_data, 'Class': labels})\n",
        "df['Lengths'] = df['Data'].apply(lambda seq: len(seq))\n",
        "print(f'Number of total data points: {df[\"Data\"].count()}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c38841",
      "metadata": {
        "id": "58c38841"
      },
      "source": [
        "## Plot some samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJ5DmWczaOZ5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJ5DmWczaOZ5",
        "outputId": "2ffff28f-33b3-4693-9fde-26051ca13b4c"
      },
      "outputs": [],
      "source": [
        "# normalize data points\n",
        "sequences = df[\"Data\"].to_numpy()\n",
        "concat = np.concatenate(sequences).reshape(-1, 1)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(concat)\n",
        "normalized_data = [scaler.transform(seq.reshape(-1, 1)).flatten() for seq in sequences]\n",
        "\n",
        "\n",
        "def plot_sequences(data, labels, num_rows, num_cols, start_idx=0, total_sequences=10, xlim_right=1200, title=\"Plots\"):\n",
        "  \n",
        "  assert 0 <= start_idx < len(data)\n",
        "  assert 0 < total_sequences and total_sequences == num_rows * num_cols\n",
        "  # assert start_idx + total_sequences < len(data)\n",
        "\n",
        "  fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 + num_rows))\n",
        "  sequences = data[start_idx : start_idx + total_sequences]\n",
        "  classes = labels[start_idx : start_idx + total_sequences]\n",
        "  sorted_idcs = np.argsort(classes)\n",
        "\n",
        "  offset = 0\n",
        "  for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "      seq = sequences[sorted_idcs[offset]]\n",
        "      clss = classes[sorted_idcs[offset]]\n",
        "      axs[i, j].plot(seq, label=clss)\n",
        "      axs[i, j].set_xlim(0, xlim_right)\n",
        "      axs[i, j].legend()\n",
        "      offset += 1\n",
        "  \n",
        "  fig.suptitle(title, fontsize=16, y=0.95)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bbaf1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_sequences(\n",
        "    normalized_data, \n",
        "    df[\"Class\"].to_numpy(), \n",
        "    num_rows=2, num_cols=4, \n",
        "    start_idx=90, total_sequences=8, \n",
        "    xlim_right=800, title=\"ECG-Signals of different classes\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8IpLwU6QaZsA",
      "metadata": {
        "id": "8IpLwU6QaZsA"
      },
      "source": [
        "## Plot class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48fb5404",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "48fb5404",
        "outputId": "478069bf-7904-4447-98af-0205957512f0"
      },
      "outputs": [],
      "source": [
        "classes = ['Class 0', 'Class 1', 'Class 2', 'Class 3']\n",
        "\n",
        "_df = df.groupby('Class').count()\n",
        "count = _df['Data'].to_numpy()\n",
        "print(f\"Class 0: {count[0]}\\tClass 1: {count[1]}\\tClass 2: {count[2]}\\tClass 3: {count[3]}\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(count, labels=classes, autopct='%1.1f%%')\n",
        "plt.title('Class Distribution of ECG Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15AMLN_eKlrY",
      "metadata": {
        "id": "15AMLN_eKlrY"
      },
      "source": [
        "## Analyze the lengths of the samples\n",
        "\n",
        "- This information is relevant for model-selection and pre-processing of data for model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235e3817",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "235e3817",
        "outputId": "a0ce9a51-6757-450f-9804-1b8236feb513"
      },
      "outputs": [],
      "source": [
        "unique_lengths, counts = np.unique(df['Lengths'], return_counts=True)\n",
        "min_val = unique_lengths.min()\n",
        "max_val = unique_lengths.max()\n",
        "mean_val = unique_lengths.mean()\n",
        "most_frequent_length = unique_lengths[np.argmax(counts)]\n",
        "\n",
        "# print(np.max(counts), counts[np.argmax(counts)])\n",
        "\n",
        "print(f'Number of all different lengths: {len(unique_lengths)}\\n')\n",
        "print(\"--- Absolute Lengths ---\")\n",
        "print(f'Min Length: {min_val}')\n",
        "print(f'Max Length: {max_val}')\n",
        "print(f'Avg. Length: {mean_val:.2f}')\n",
        "print(f'Most Frequent Length: {most_frequent_length} with total of {np.max(counts)} occurences\\n')\n",
        "\n",
        "print(\"--- Class dependant Lengths ---\")\n",
        "mins = df.groupby(\"Class\")[\"Lengths\"].min()\n",
        "maxs = df.groupby(\"Class\")[\"Lengths\"].max()\n",
        "avgs = df.groupby(\"Class\")[\"Lengths\"].mean()\n",
        "print(f\"Class 0  --  Min-Length: {mins[0]}  -  Max-Length: {maxs[0]}  -  Avg-Length: {int(avgs[0])}\")\n",
        "print(f\"Class 1  --  Min-Length: {mins[1]}  -  Max-Length: {maxs[1]}  -  Avg-Length: {int(avgs[1])}\")\n",
        "print(f\"Class 2  --  Min-Length: {mins[2]}  -  Max-Length: {maxs[2]}  -  Avg-Length: {int(avgs[2])}\")\n",
        "print(f\"Class 3  --  Min-Length: {mins[3]}  -  Max-Length: {maxs[3]}  -  Avg-Length: {int(avgs[3])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tzOn7NbYPzcr",
      "metadata": {
        "id": "tzOn7NbYPzcr"
      },
      "source": [
        "## Compute descriptive statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1900880",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_class_statistics(df: pd.DataFrame, class_: int):\n",
        "\n",
        "    group = df.groupby(\"Class\")[\"Data\"].get_group(class_)\n",
        "    all_stats = np.zeros(8)\n",
        "\n",
        "    for series in group:\n",
        "        stats = pd.Series(series).describe()\n",
        "        all_stats += stats\n",
        "\n",
        "    all_stats /= group.shape[0]\n",
        "    return all_stats\n",
        "\n",
        "# df.groupby(\"Class\")[\"Data\"].apply()\n",
        "count0, mean0, std0, min0, quant0_25, quant0_50, quant0_75, max0 = get_class_statistics(df, 0)\n",
        "count1, mean1, std1, min1, quant1_25, quant1_50, quant1_75, max1 = get_class_statistics(df, 1)\n",
        "count2, mean2, std2, min2, quant2_25, quant2_50, quant2_75, max2 = get_class_statistics(df, 2)\n",
        "count3, mean3, std3, min3, quant3_25, quant3_50, quant3_75, max3 = get_class_statistics(df, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccffdb5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"-- Class 0 -- \\n\\nCount: {int(count0)}\\nMean: {mean0:.2f}\\nMin: {min0:.2f}\\nMax: {max0:.2f}\\n25% Quantile: {quant0_25:.2f}\\n50% Quantile: {quant0_50:.2f}\\n75% Quantile: {quant0_75:.2f}\\n\")\n",
        "print(f\"-- Class 1 -- \\n\\nCount: {int(count1)}\\nMean: {mean1:.2f}\\nMin: {min1:.2f}\\nMax: {max1:.2f}\\n25% Quantile: {quant1_25:.2f}\\n50% Quantile: {quant1_50:.2f}\\n75% Quantile: {quant1_75:.2f}\\n\")\n",
        "print(f\"-- Class 2 -- \\n\\nCount: {int(count2)}\\nMean: {mean2:.2f}\\nMin: {min2:.2f}\\nMax: {max2:.2f}\\n25% Quantile: {quant2_25:.2f}\\n50% Quantile: {quant2_50:.2f}\\n75% Quantile: {quant2_75:.2f}\\n\")\n",
        "print(f\"-- Class 3 -- \\n\\nCount: {int(count3)}\\nMean: {mean3:.2f}\\nMin: {min3:.2f}\\nMax: {max3:.2f}\\n25% Quantile: {quant3_25:.2f}\\n50% Quantile: {quant3_50:.2f}\\n75% Quantile: {quant3_75:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfebDG92awbL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfebDG92awbL",
        "outputId": "ddd5b1d3-06ce-496c-8388-6942b418c572"
      },
      "outputs": [],
      "source": [
        "# plot mean distribution\n",
        "max_lengths = 18286\n",
        "df_norm = pd.DataFrame({\"Data\": normalized_data, \"Class\": df[\"Class\"]})\n",
        "df_norm[\"Data\"] = df_norm[\"Data\"].apply(lambda sample: np.pad(sample, (0, max_lengths - sample.shape[0])))\n",
        "mean_signals = [np.mean(np.stack(df_norm[df_norm[\"Class\"] == i][\"Data\"].to_numpy()), axis=0) for i in range(len(classes))]\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for mean_signal, clss in zip(mean_signals, classes):\n",
        "    plt.plot(mean_signal, label=clss)\n",
        "\n",
        "plt.title(\"Averaged signals per class\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.xlim(0, 600)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab24f684",
      "metadata": {},
      "source": [
        "### Create Training and Validation Split (80/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38daa16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create training and validation split\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(ecg_data, labels, test_size=0.2, random_state=0, stratify=labels)\n",
        "\n",
        "# X_train = [np.array(x, dtype=np.float32) for x in X_train]\n",
        "# X_validation = [np.array(x, dtype=np.float32) for x in X_validation]\n",
        "train_lengths = [x.shape[0] for x in X_train]\n",
        "validation_lengths = [x.shape[0] for x in X_validation]\n",
        "\n",
        "\n",
        "# verify that the training and validation split is proportional to the original class distribution\n",
        "counter0 = Counter(labels)\n",
        "counter1 = Counter(y_train)\n",
        "counter2 = Counter(y_validation)\n",
        "\n",
        "total0 = len(labels)\n",
        "ratios0 = np.array([counter0[0]/total0, counter0[1]/total0, counter0[2]/total0, counter0[3]/total0]) # original dataset ratios\n",
        "\n",
        "total1 = counter1[0] + counter1[1] + counter1[2] + counter1[3]\n",
        "ratios1 = np.array([counter1[0]/total1, counter1[1]/total1, counter1[2]/total1, counter1[3]/total1]) # training dataset ratios\n",
        "\n",
        "total2 = counter2[0] + counter2[1] + counter2[2] + counter2[3]\n",
        "ratios2 = np.array([counter2[0]/total2, counter2[1]/total2, counter2[2]/total2, counter2[3]/total2]) # validation dataset ratios\n",
        "\n",
        "assert np.allclose(ratios0, ratios1, atol=1e-3)\n",
        "assert np.allclose(ratios0, ratios2, atol=1e-3)\n",
        "assert np.allclose(ratios1, ratios2, atol=1e-3)\n",
        "\n",
        "\n",
        "# prepare data for training\n",
        "# X_train, y_train = torch.tensor(zero_pad_data(X_train, max_val), dtype=torch.float32), torch.tensor(y_train)\n",
        "# X_validation, y_validation = torch.tensor(zero_pad_data(X_validation, max_val), dtype=torch.float32), torch.tensor(y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f982d853",
      "metadata": {},
      "source": [
        "# Task 1.2 - Modeling and Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb92866",
      "metadata": {},
      "source": [
        "Define the actual model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e451b59",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ECGNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(ECGNN, self).__init__()\n",
        "\n",
        "        # define convolutional layers (Conv2d) to extract local patterns in sequences\n",
        "        # input --> stft --> Conv2d --> BatchNorm2d --> ReLu --> MaxPool2d\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # define LSTM to extract global patterns in sequences\n",
        "        self.rnn = nn.LSTM(832, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
        "        \n",
        "        n_fft, hop_length = 128, 64\n",
        "        window = torch.hann_window(n_fft).to(x.device)\n",
        "        x = torch.stft(x, n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True).abs()\n",
        "\n",
        "        x = x.unsqueeze(dim=1)\n",
        "        x = self.mp(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.mp(self.relu(self.bn2(self.conv2(x)))) # output-shape: [64, 64, 62, 33] (batch_size, channels, freq_bins, time_bins)\n",
        "        \n",
        "        lengths = (lengths / hop_length).floor()\n",
        "        lengths = lengths - 4\n",
        "        lengths = (lengths / 4).floor()\n",
        "        \n",
        "        # x = torch.log2(x + 1e-6)\n",
        "        \n",
        "        x = x.view(x.size(0), -1, x.size(3))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lengths = lengths.clamp(max=x.size(1)).long()\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        _, (hn, cn) = self.rnn(x)\n",
        "        x = self.fc(hn[-1])\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b9ba68",
      "metadata": {},
      "source": [
        "Define the training and test routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cb8463",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "        model: nn.Module, \n",
        "        device: torch.device, \n",
        "        train_loader: torch.utils.data.DataLoader, \n",
        "        criterion: nn.CrossEntropyLoss,\n",
        "        optimizer: optim.Optimizer, \n",
        "        epoch: int\n",
        "):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target, lengths) in enumerate(train_loader):\n",
        "        # forward pass\n",
        "        data, target, lengths = data.to(device), target.to(device), lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data, lengths)\n",
        "        # backward pass\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            data_processed = batch_idx * len(data)\n",
        "            total_data = len(train_loader.dataset)\n",
        "            progress = (100. * batch_idx) / len(train_loader)\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)] \\t Loss: {:.6f}\".format(\n",
        "                epoch, data_processed, total_data, progress, loss.item()\n",
        "            ), end=\"\\r\")\n",
        "\n",
        "def validate(\n",
        "        model: nn.Module, \n",
        "        device: torch.device, \n",
        "        validation_loader: torch.utils.data.DataLoader,\n",
        "        criterion: nn.CrossEntropyLoss\n",
        "):\n",
        "    model.eval()\n",
        "    validation_loss = 0.\n",
        "    correct = 0\n",
        "    correct_per_class = torch.zeros(4)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target, lengths) in enumerate(validation_loader):\n",
        "            # do forward pass\n",
        "            data, target, lengths = data.to(device), target.to(device), lengths.to(device)\n",
        "            output = model(data, lengths)\n",
        "            validation_loss += criterion(output, target).item() * len(data)\n",
        "            # compute correct predictions\n",
        "            preds = output.argmax(dim=1, keepdim=False)\n",
        "            mask = preds == target\n",
        "            correct_preds = torch.count_nonzero(mask).item()\n",
        "            correct += correct_preds\n",
        "            for t, m in zip(target, mask):\n",
        "                if m: correct_per_class[t] += 1\n",
        "\n",
        "    total_data = len(validation_loader.dataset)\n",
        "    validation_loss /= total_data\n",
        "    progress = (100. * correct)  / len(validation_loader.dataset)\n",
        "    \n",
        "    print(\"Validation Set: Average Loss {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "        validation_loss, correct, total_data, progress\n",
        "    ), end=\"\\r\")\n",
        "    print(\"Class 0: {:.0f}/{}\\tClass 1: {:.0f}/{}\\tClass 3: {:.0f}/{}\\tClass 3: {:.0f}/{}\\n\".format(\n",
        "        correct_per_class[0], counter2[0], correct_per_class[1], counter2[1], correct_per_class[2], counter2[2], correct_per_class[3], counter2[3],\n",
        "    ))\n",
        "    \n",
        "    return validation_loss\n",
        "\n",
        "def evaluate_model(\n",
        "        model: nn.Module,\n",
        "        device: torch.device,\n",
        "        test_loader: torch.utils.data.DataLoader,\n",
        "        y_true: torch.Tensor,\n",
        "        target_names: list\n",
        "):\n",
        "    model.eval()\n",
        "    y_pred = torch.empty(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            # forward pass\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            # compute correct predictions\n",
        "            new_pred = output.argmax(dim=1, keepdim=False)\n",
        "            y_pred = torch.cat((y_pred, new_pred))\n",
        "    \n",
        "    classification_report(y_true, y_pred, target_names=target_names)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d943a005",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples_per_class = torch.from_numpy(df.groupby(\"Class\").count()[\"Data\"].to_numpy())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# hyperparameters\n",
        "weights =  num_samples_per_class.sum() / (num_samples_per_class * len(num_samples_per_class))\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "\n",
        "input_size = 129\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "output_size = 4\n",
        "\n",
        "# initialize model and define necessary objects for training\n",
        "model = ECGNN(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights.float())\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # try lr: 0.01 or 0.001 and other weight decay\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=4)\n",
        "\n",
        "training_data = TimeSeriesDataset(X_train, y_train, train_lengths)\n",
        "validation_data = TimeSeriesDataset(X_validation, y_validation, validation_lengths)\n",
        "\n",
        "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# training and test loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, criterion, optimizer, epoch)\n",
        "    valid_loss = validate(model, device, validation_loader, criterion)\n",
        "    scheduler.step(valid_loss)\n",
        "    \n",
        "evaluate_model(model, device, validation_loader, y_validation, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "26c19b74",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n-- mit log2\\nValidation Set: Average Loss 1.2355, Accuracy: 660/1236 (53%)\\nValidation Set: Average Loss 1.2226, Accuracy: 280/1236 (23%)\\nValidation Set: Average Loss 1.1676, Accuracy: 222/1236 (18%)\\nValidation Set: Average Loss 1.2645, Accuracy: 150/1236 (12%)\\nValidation Set: Average Loss 1.2164, Accuracy: 255/1236 (21%)\\nValidation Set: Average Loss 1.1568, Accuracy: 392/1236 (32%)\\nValidation Set: Average Loss 1.1648, Accuracy: 411/1236 (33%)\\nValidation Set: Average Loss 1.1716, Accuracy: 292/1236 (24%)\\n\\n-- ohne log2\\nValidation Set: Average Loss 1.1630, Accuracy: 626/1236 (51%)\\nValidation Set: Average Loss 1.1218, Accuracy: 411/1236 (33%)\\nValidation Set: Average Loss 1.1386, Accuracy: 362/1236 (29%)\\nValidation Set: Average Loss 1.0890, Accuracy: 675/1236 (55%)\\nValidation Set: Average Loss 1.1030, Accuracy: 527/1236 (43%)\\nValidation Set: Average Loss 1.1014, Accuracy: 662/1236 (54%)\\nValidation Set: Average Loss 1.0843, Accuracy: 528/1236 (43%)\\nValidation Set: Average Loss 1.0490, Accuracy: 697/1236 (56%)\\n\\n\\n-- mit min max scaler\\nValidation Set: Average Loss 1.3694, Accuracy: 695/1236 (56%)\\nValidation Set: Average Loss 1.3279, Accuracy: 699/1236 (57%)\\nValidation Set: Average Loss 1.2265, Accuracy: 583/1236 (47%)\\nValidation Set: Average Loss 1.1812, Accuracy: 241/1236 (19%)\\nValidation Set: Average Loss 1.1525, Accuracy: 258/1236 (21%)\\nValidation Set: Average Loss 1.1157, Accuracy: 339/1236 (27%)\\nValidation Set: Average Loss 1.0987, Accuracy: 408/1236 (33%)\\n\\n-- ohne min max scaler\\nValidation Set: Average Loss 1.3544, Accuracy: 273/1236 (22%)\\nValidation Set: Average Loss 1.3171, Accuracy: 334/1236 (27%)\\nValidation Set: Average Loss 1.2616, Accuracy: 354/1236 (29%)\\nValidation Set: Average Loss 1.2030, Accuracy: 344/1236 (28%)\\nValidation Set: Average Loss 1.1772, Accuracy: 406/1236 (33%)\\nValidation Set: Average Loss 1.1630, Accuracy: 471/1236 (38%)\\nValidation Set: Average Loss 1.1514, Accuracy: 601/1236 (49%)\\nValidation Set: Average Loss 1.1464, Accuracy: 469/1236 (38%)\\nValidation Set: Average Loss 1.1340, Accuracy: 594/1236 (48%)\\nValidation Set: Average Loss 1.1242, Accuracy: 637/1236 (52%)\\nValidation Set: Average Loss 1.1233, Accuracy: 629/1236 (51%)\\nValidation Set: Average Loss 1.1218, Accuracy: 635/1236 (51%)\\nValidation Set: Average Loss 1.1187, Accuracy: 636/1236 (51%)\\nValidation Set: Average Loss 1.1180, Accuracy: 639/1236 (52%)\\nValidation Set: Average Loss 1.1161, Accuracy: 649/1236 (53%)\\nValidation Set: Average Loss 1.1168, Accuracy: 648/1236 (52%)\\nValidation Set: Average Loss 1.1173, Accuracy: 647/1236 (52%)\\nValidation Set: Average Loss 1.1156, Accuracy: 633/1236 (51%)\\nValidation Set: Average Loss 1.1143, Accuracy: 635/1236 (51%)\\nValidation Set: Average Loss 1.1137, Accuracy: 634/1236 (51%)\\n\\n\\n-------------------------------------------\\nnum_epochs = 20\\nbatch_size = 64\\nlr = 0.001\\nweight_decay = 0.0001\\nstep_size = 10\\ngamma = 0.1\\n\\ninput_size = 257\\nhidden_size = 64\\nnum_layers = 2\\noutput_size = 4\\n\\nValidation Set: Average Loss 1.0932, Accuracy: 640/1236 (52%)\\nValidation Set: Average Loss 1.0989, Accuracy: 624/1236 (50%)\\nValidation Set: Average Loss 1.0565, Accuracy: 671/1236 (54%)\\nValidation Set: Average Loss 1.0006, Accuracy: 595/1236 (48%)\\nValidation Set: Average Loss 0.9564, Accuracy: 638/1236 (52%)\\nValidation Set: Average Loss 0.9267, Accuracy: 671/1236 (54%)\\nValidation Set: Average Loss 1.0927, Accuracy: 783/1236 (63%)\\nValidation Set: Average Loss 0.9567, Accuracy: 707/1236 (57%)\\nValidation Set: Average Loss 0.9516, Accuracy: 733/1236 (59%)\\nValidation Set: Average Loss 1.0527, Accuracy: 774/1236 (63%)\\nValidation Set: Average Loss 0.9122, Accuracy: 789/1236 (64%)\\nValidation Set: Average Loss 0.9102, Accuracy: 770/1236 (62%)\\nValidation Set: Average Loss 0.9324, Accuracy: 778/1236 (63%)\\nValidation Set: Average Loss 0.9331, Accuracy: 779/1236 (63%)\\nValidation Set: Average Loss 0.9604, Accuracy: 816/1236 (66%)\\nValidation Set: Average Loss 0.9579, Accuracy: 813/1236 (66%)\\nValidation Set: Average Loss 0.9555, Accuracy: 803/1236 (65%)\\nValidation Set: Average Loss 0.9394, Accuracy: 786/1236 (64%)\\nValidation Set: Average Loss 0.9620, Accuracy: 800/1236 (65%)\\nValidation Set: Average Loss 0.9476, Accuracy: 797/1236 (64%)\\n\\n\\n-- bisher beste mit 78% accuracy\\n\\nweights =  num_samples_per_class.max() / num_samples_per_class\\nnum_epochs = 20\\nbatch_size = 64\\nlr = 0.001\\nweight_decay = 0.0001\\nstep_size = 10\\ngamma = 0.1\\n\\ninput_size = 129\\nhidden_size = 128\\nnum_layers = 2\\noutput_size = 4\\n'"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "-- mit log2\n",
        "Validation Set: Average Loss 1.2355, Accuracy: 660/1236 (53%)\n",
        "Validation Set: Average Loss 1.2226, Accuracy: 280/1236 (23%)\n",
        "Validation Set: Average Loss 1.1676, Accuracy: 222/1236 (18%)\n",
        "Validation Set: Average Loss 1.2645, Accuracy: 150/1236 (12%)\n",
        "Validation Set: Average Loss 1.2164, Accuracy: 255/1236 (21%)\n",
        "Validation Set: Average Loss 1.1568, Accuracy: 392/1236 (32%)\n",
        "Validation Set: Average Loss 1.1648, Accuracy: 411/1236 (33%)\n",
        "Validation Set: Average Loss 1.1716, Accuracy: 292/1236 (24%)\n",
        "\n",
        "-- ohne log2\n",
        "Validation Set: Average Loss 1.1630, Accuracy: 626/1236 (51%)\n",
        "Validation Set: Average Loss 1.1218, Accuracy: 411/1236 (33%)\n",
        "Validation Set: Average Loss 1.1386, Accuracy: 362/1236 (29%)\n",
        "Validation Set: Average Loss 1.0890, Accuracy: 675/1236 (55%)\n",
        "Validation Set: Average Loss 1.1030, Accuracy: 527/1236 (43%)\n",
        "Validation Set: Average Loss 1.1014, Accuracy: 662/1236 (54%)\n",
        "Validation Set: Average Loss 1.0843, Accuracy: 528/1236 (43%)\n",
        "Validation Set: Average Loss 1.0490, Accuracy: 697/1236 (56%)\n",
        "\n",
        "\n",
        "-- mit min max scaler\n",
        "Validation Set: Average Loss 1.3694, Accuracy: 695/1236 (56%)\n",
        "Validation Set: Average Loss 1.3279, Accuracy: 699/1236 (57%)\n",
        "Validation Set: Average Loss 1.2265, Accuracy: 583/1236 (47%)\n",
        "Validation Set: Average Loss 1.1812, Accuracy: 241/1236 (19%)\n",
        "Validation Set: Average Loss 1.1525, Accuracy: 258/1236 (21%)\n",
        "Validation Set: Average Loss 1.1157, Accuracy: 339/1236 (27%)\n",
        "Validation Set: Average Loss 1.0987, Accuracy: 408/1236 (33%)\n",
        "\n",
        "-- ohne min max scaler\n",
        "Validation Set: Average Loss 1.3544, Accuracy: 273/1236 (22%)\n",
        "Validation Set: Average Loss 1.3171, Accuracy: 334/1236 (27%)\n",
        "Validation Set: Average Loss 1.2616, Accuracy: 354/1236 (29%)\n",
        "Validation Set: Average Loss 1.2030, Accuracy: 344/1236 (28%)\n",
        "Validation Set: Average Loss 1.1772, Accuracy: 406/1236 (33%)\n",
        "Validation Set: Average Loss 1.1630, Accuracy: 471/1236 (38%)\n",
        "Validation Set: Average Loss 1.1514, Accuracy: 601/1236 (49%)\n",
        "Validation Set: Average Loss 1.1464, Accuracy: 469/1236 (38%)\n",
        "Validation Set: Average Loss 1.1340, Accuracy: 594/1236 (48%)\n",
        "Validation Set: Average Loss 1.1242, Accuracy: 637/1236 (52%)\n",
        "Validation Set: Average Loss 1.1233, Accuracy: 629/1236 (51%)\n",
        "Validation Set: Average Loss 1.1218, Accuracy: 635/1236 (51%)\n",
        "Validation Set: Average Loss 1.1187, Accuracy: 636/1236 (51%)\n",
        "Validation Set: Average Loss 1.1180, Accuracy: 639/1236 (52%)\n",
        "Validation Set: Average Loss 1.1161, Accuracy: 649/1236 (53%)\n",
        "Validation Set: Average Loss 1.1168, Accuracy: 648/1236 (52%)\n",
        "Validation Set: Average Loss 1.1173, Accuracy: 647/1236 (52%)\n",
        "Validation Set: Average Loss 1.1156, Accuracy: 633/1236 (51%)\n",
        "Validation Set: Average Loss 1.1143, Accuracy: 635/1236 (51%)\n",
        "Validation Set: Average Loss 1.1137, Accuracy: 634/1236 (51%)\n",
        "\n",
        "\n",
        "-------------------------------------------\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "\n",
        "input_size = 257\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 4\n",
        "\n",
        "Validation Set: Average Loss 1.0932, Accuracy: 640/1236 (52%)\n",
        "Validation Set: Average Loss 1.0989, Accuracy: 624/1236 (50%)\n",
        "Validation Set: Average Loss 1.0565, Accuracy: 671/1236 (54%)\n",
        "Validation Set: Average Loss 1.0006, Accuracy: 595/1236 (48%)\n",
        "Validation Set: Average Loss 0.9564, Accuracy: 638/1236 (52%)\n",
        "Validation Set: Average Loss 0.9267, Accuracy: 671/1236 (54%)\n",
        "Validation Set: Average Loss 1.0927, Accuracy: 783/1236 (63%)\n",
        "Validation Set: Average Loss 0.9567, Accuracy: 707/1236 (57%)\n",
        "Validation Set: Average Loss 0.9516, Accuracy: 733/1236 (59%)\n",
        "Validation Set: Average Loss 1.0527, Accuracy: 774/1236 (63%)\n",
        "Validation Set: Average Loss 0.9122, Accuracy: 789/1236 (64%)\n",
        "Validation Set: Average Loss 0.9102, Accuracy: 770/1236 (62%)\n",
        "Validation Set: Average Loss 0.9324, Accuracy: 778/1236 (63%)\n",
        "Validation Set: Average Loss 0.9331, Accuracy: 779/1236 (63%)\n",
        "Validation Set: Average Loss 0.9604, Accuracy: 816/1236 (66%)\n",
        "Validation Set: Average Loss 0.9579, Accuracy: 813/1236 (66%)\n",
        "Validation Set: Average Loss 0.9555, Accuracy: 803/1236 (65%)\n",
        "Validation Set: Average Loss 0.9394, Accuracy: 786/1236 (64%)\n",
        "Validation Set: Average Loss 0.9620, Accuracy: 800/1236 (65%)\n",
        "Validation Set: Average Loss 0.9476, Accuracy: 797/1236 (64%)\n",
        "\n",
        "\n",
        "-- bisher beste mit 78% accuracy\n",
        "\n",
        "weights =  num_samples_per_class.max() / num_samples_per_class\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "\n",
        "input_size = 129\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 4\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.11.5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
